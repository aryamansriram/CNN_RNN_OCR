{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test_valid(root_dir,folder):\n",
    "    '''Converts train test and valid lst files\n",
    "    to dataframe\n",
    "    args: root_dir: root directory\n",
    "    folder: name of file in which the image filenames are\n",
    "    '''\n",
    "    \n",
    "    train_dict = {}\n",
    "    train_dict[\"image\"] = []\n",
    "    train_dict[\"latex_line\"] = []\n",
    "    with open(root_dir+folder) as f:\n",
    "        arr = f.readlines()\n",
    "    for record in arr:\n",
    "        temp = record.split()\n",
    "        train_dict[\"image\"].append(temp[0])\n",
    "        train_dict[\"latex_line\"].append(temp[1])\n",
    "    train_df = pd.DataFrame(train_dict)\n",
    "    return train_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>latex_line</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7944775fc9.png</td>\n",
       "      <td>32771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>78228211ca.png</td>\n",
       "      <td>32772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15b9034ba8.png</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6968dfca15.png</td>\n",
       "      <td>14185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6cead0df53.png</td>\n",
       "      <td>98321</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            image latex_line\n",
       "0  7944775fc9.png      32771\n",
       "1  78228211ca.png      32772\n",
       "2  15b9034ba8.png         11\n",
       "3  6968dfca15.png      14185\n",
       "4  6cead0df53.png      98321"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root_dir = \"data/\"\n",
    "train = \"train.lst\"\n",
    "test = \"test.lst\"\n",
    "valid = \"valid.lst\"\n",
    "train_df = get_train_test_valid(root_dir,train)\n",
    "val_df = get_train_test_valid(root_dir,test)\n",
    "test_df = get_train_test_valid(root_dir,valid)\n",
    "val_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_latex(root_dir,latex_folder,df):\n",
    "    '''Added latex expression to dataframe\n",
    "    args: root_dir: Root directory\n",
    "          latex_folder: Folder containing latex expressions\n",
    "          df: train/test/valid dataframe in question\n",
    "    returns: new dataframe with latex expressions added\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    with open(root_dir+latex_folder) as f:\n",
    "        arr = f.readlines()\n",
    "    latex = []\n",
    "    for index in df.latex_line.values:\n",
    "\n",
    "        latex.append(arr[(int)(index)])\n",
    "    df[\"latex_exp\"] = latex\n",
    "    return df\n",
    "\n",
    "train_df = add_latex(root_dir,\"formulas.norm.lst\",train_df)\n",
    "val_df = add_latex(root_dir,\"formulas.norm.lst\",val_df)\n",
    "test_df = add_latex(root_dir,\"formulas.norm.lst\",test_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(\"data/latex_vocab.txt\") as f:\n",
    "    vocab = f.read().split(\"\\n\")\n",
    "word2index = {\"SOS\":0,\"EOS\":1}\n",
    "for i in range(len(vocab)):\n",
    "    word2index[vocab[i]] = i+2\n",
    "index2word = {0:\"SOS\",1:\"EOS\"}\n",
    "for i in range(len(vocab)):\n",
    "    index2word[i+2] = vocab[i]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>latex_line</th>\n",
       "      <th>latex_exp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>60ee748793.png</td>\n",
       "      <td>1</td>\n",
       "      <td>d s ^ { 2 } = ( 1 - { \\frac { q c o s \\theta }...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>66667cee5b.png</td>\n",
       "      <td>2</td>\n",
       "      <td>\\widetilde \\gamma _ { \\mathrm { h o p f } } \\s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1cbb05a562.png</td>\n",
       "      <td>3</td>\n",
       "      <td>( { \\cal L } _ { a } g ) _ { i j } = 0 , \\ \\ \\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ed164cc822.png</td>\n",
       "      <td>4</td>\n",
       "      <td>S _ { s t a t } = 2 \\pi \\sqrt { N _ { 5 } ^ { ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>e265f9dc6b.png</td>\n",
       "      <td>5</td>\n",
       "      <td>\\hat { N } _ { 3 } = \\sum \\sp f _ { j = 1 } a ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            image latex_line  \\\n",
       "0  60ee748793.png          1   \n",
       "1  66667cee5b.png          2   \n",
       "2  1cbb05a562.png          3   \n",
       "3  ed164cc822.png          4   \n",
       "4  e265f9dc6b.png          5   \n",
       "\n",
       "                                           latex_exp  \n",
       "0  d s ^ { 2 } = ( 1 - { \\frac { q c o s \\theta }...  \n",
       "1  \\widetilde \\gamma _ { \\mathrm { h o p f } } \\s...  \n",
       "2  ( { \\cal L } _ { a } g ) _ { i j } = 0 , \\ \\ \\...  \n",
       "3  S _ { s t a t } = 2 \\pi \\sqrt { N _ { 5 } ^ { ...  \n",
       "4  \\hat { N } _ { 3 } = \\sum \\sp f _ { j = 1 } a ...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader,Dataset,SequentialSampler\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import cv2\n",
    "\n",
    "class OCR_Dataset(Dataset):\n",
    "    '''Custom dataset for latex images'''\n",
    "    def __init__(self,csv,root_dir,max_len=150,transforms=None):\n",
    "        self.csv = csv\n",
    "        self.transforms = transforms\n",
    "        self.root_dir = root_dir\n",
    "        self.max_len = max_len\n",
    "    def __len__(self):\n",
    "        return len(self.csv)\n",
    "    def __getitem__(self,idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx.to_list()\n",
    "        req = self.csv.iloc[idx]\n",
    "        img_name = req.image\n",
    "        latex = \"SOS \"+req.latex_exp+\" EOS\"\n",
    "        \n",
    "        encoding = [word2index[i] for i in latex.split()]\n",
    "        for i in range(len(encoding),self.max_len):\n",
    "            encoding.append(0)\n",
    "        path = self.root_dir+img_name\n",
    "        img = cv2.imread(path)\n",
    "        if self.transforms:\n",
    "            img = self.transforms(img)\n",
    "        sample = {\"img\":img,\"label\":latex,\"encoding\":torch.tensor(encoding)}\n",
    "        return sample\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ocr_dset = OCR_Dataset(train_df,root_dir = \"data/images_processed/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 'SOS d s ^ { 2 } = ( 1 - { \\\\frac { q c o s \\\\theta } { r } } ) ^ { \\\\frac { 2 } { 1 + \\\\alpha ^ { 2 } } } \\\\lbrace d r ^ { 2 } + r ^ { 2 } d \\\\theta ^ { 2 } + r ^ { 2 } s i n ^ { 2 } \\\\theta d \\\\varphi ^ { 2 } \\\\rbrace - { \\\\frac { d t ^ { 2 } } { ( 1 - { \\\\frac { q c o s \\\\theta } { r } } ) ^ { \\\\frac { 2 } { 1 + \\\\alpha ^ { 2 } } } } } \\\\, .\\n EOS', 'encoding': tensor([  0, 470, 488, 463, 497,  23, 499,  37,   6,  22,  11, 497, 207, 497,\n",
      "        486, 468, 483, 488, 418, 499, 497, 487, 499, 499,   7, 463, 497, 207,\n",
      "        497,  23, 499, 497,  22,   9, 123, 463, 497,  23, 499, 499, 499, 242,\n",
      "        470, 487, 463, 497,  23, 499,   9, 487, 463, 497,  23, 499, 470, 418,\n",
      "        463, 497,  23, 499,   9, 487, 463, 497,  23, 499, 488, 475, 482, 463,\n",
      "        497,  23, 499, 418, 470, 436, 463, 497,  23, 499, 346,  11, 497, 207,\n",
      "        497, 470, 489, 463, 497,  23, 499, 499, 497,   6,  22,  11, 497, 207,\n",
      "        497, 486, 468, 483, 488, 418, 499, 497, 487, 499, 499,   7, 463, 497,\n",
      "        207, 497,  23, 499, 497,  22,   9, 123, 463, 497,  23, 499, 499, 499,\n",
      "        499, 499,  76,  14,   1,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0]), 'img': array([[[255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        ...,\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255]],\n",
      "\n",
      "       [[255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        ...,\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255]],\n",
      "\n",
      "       [[255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        ...,\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        ...,\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255]],\n",
      "\n",
      "       [[255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        ...,\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255]],\n",
      "\n",
      "       [[255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        ...,\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255]]], dtype=uint8)}\n"
     ]
    }
   ],
   "source": [
    "sample = ocr_dset[0]\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1500x1500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(figsize=(15,15))\n",
    "plt.imshow(sample[\"img\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    '''Definition of the convnet part of the paper'''\n",
    "    def __init__(self):\n",
    "        super(ConvNet,self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "                    nn.Conv2d(3,512,kernel_size=3,stride=1),\n",
    "                    nn.BatchNorm2d(512)\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "                    nn.Conv2d(512,512,kernel_size=3,stride=1,padding=1),\n",
    "                    nn.BatchNorm2d(512),\n",
    "                    nn.MaxPool2d(kernel_size=(1,2),stride=(1,2))\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "                    nn.Conv2d(512,256,kernel_size=3,stride=1,padding=1),\n",
    "                    nn.MaxPool2d(kernel_size=(2,1),stride=(2,1))\n",
    "        )\n",
    "        self.conv4 = nn.Sequential(\n",
    "                    nn.Conv2d(256,256,kernel_size=3,stride=1,padding=1),\n",
    "                    nn.BatchNorm2d(256)\n",
    "        )\n",
    "        self.conv5 = nn.Sequential(\n",
    "                    nn.Conv2d(256,128,kernel_size=3,stride=1,padding=1),\n",
    "                    nn.MaxPool2d(kernel_size=2,stride=2)\n",
    "        )\n",
    "        self.final = nn.Sequential(\n",
    "                nn.Conv2d(128,64,kernel_size=3,stride=1,padding=1),\n",
    "                nn.MaxPool2d(kernel_size=2,stride=2,padding=1)\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = F.relu(self.conv5(x))\n",
    "        out = F.relu(self.final(x))\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class rowEncoder(nn.Module):\n",
    "    '''Definition of rowEncoder part of the network'''\n",
    "    def __init__(self,inp_dim,hidden_dim):\n",
    "        super(rowEncoder,self).__init__()\n",
    "        self.inp_dim = inp_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.lstm = nn.LSTM(self.inp_dim,self.hidden_dim,num_layers=1,batch_first=False,bidirectional=True)\n",
    "    \n",
    "    def forward(self,x,hidden):\n",
    "    \n",
    "        outputs,(hidden,cell) = self.lstm(x,hidden)\n",
    "    \n",
    "        hidden = torch.tanh(torch.cat((hidden[0],hidden[1]),dim=1))\n",
    "\n",
    "        return outputs,hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size=1):\n",
    "        return (torch.zeros(2, batch_size, self.hidden_dim),\n",
    "                torch.zeros(2, batch_size, self.hidden_dim))\n",
    "                \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Decoder#####\n",
    "class BahadanauDecoder(nn.Module):\n",
    "    \n",
    "    def __init__(self,hidden_size,output_size,n_layers=1):\n",
    "        super(BahadanauDecoder,self).__init__()\n",
    "        self.hidden_size=hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.embedding = nn.Embedding(500, self.hidden_size*2)\n",
    "        self.fc_hidden = nn.Linear(self.hidden_size,self.hidden_size)\n",
    "        self.fc_encoder = nn.Linear(self.hidden_size*2,self.hidden_size)\n",
    "        self.weight = nn.Linear(self.hidden_size,1)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size*2,self.hidden_size)\n",
    "        self.lstm = nn.LSTM(self.hidden_size*4,self.hidden_size,batch_first=True)\n",
    "        self.classifier = nn.Linear(self.hidden_size,self.output_size)\n",
    "    \n",
    "    \n",
    "    def forward(self,inputs,hidden,encoder_outputs):\n",
    "        encoder_outputs = encoder_outputs.squeeze()\n",
    "        embedded = self.embedding(inputs).view(1, -1)\n",
    "        #print(\"enc ops\",encoder_outputs.size())\n",
    "        #print(\"Hidden: \",hidden[0].size())\n",
    "        #print(encoder_outputs.size())\n",
    "        x = torch.tanh(self.fc_hidden(hidden[0])+self.fc_encoder(encoder_outputs))\n",
    "        #print(\"X: \",x.size())\n",
    "        x = x.permute(1,0,2)\n",
    "        #print(\"enc outputs mul: \",self.fc_encoder(encoder_outputs).size())\n",
    "        #print(\"Hidden Mul: \",self.fc_hidden(hidden[0]).size())\n",
    "        #print(\"weights: \",self.weight.size())\n",
    "        \n",
    "        #scores = x.bmm(self.weight.unsqueeze(2)\n",
    "        scores = self.weight(x)\n",
    "        #print(\"scores:\",scores.size())\n",
    "        attn_weights = F.softmax(scores,dim=1)\n",
    "        \n",
    "        context_vector = torch.bmm(attn_weights.permute(0,2,1),encoder_outputs.permute(1,0,2))\n",
    "        \n",
    "        #print(embedded.size())\n",
    "        #print(\"CV: \",context_vector.size())\n",
    "        #print(embedded.long().repeat(13,1).size())\n",
    "        output = torch.cat((embedded.repeat(13,1), context_vector.squeeze(1)), 1).unsqueeze(0)\n",
    "       \n",
    "        \n",
    "        hid = hidden\n",
    "        output = output.permute(1,0,2)\n",
    "        #print(\"OP\",output.size())\n",
    "        #print(\"HID[0]\",hid[0].size())\n",
    "        output, hidden = self.lstm(output,hid)\n",
    "        print(\"Inner output: \",output.size())\n",
    "        output = F.log_softmax(self.classifier(output[0]), dim=1)\n",
    "        return output, hidden, attn_weights\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        return (torch.zeros(1,13,128),torch.zeros(1,13,128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Define dataloaders###\n",
    "from torchvision import transforms\n",
    "trans = transforms.Compose([transforms.ToPILImage(),transforms.Resize((100,240)),transforms.ToTensor()])\n",
    "train_dataset = OCR_Dataset(train_df,root_dir = \"data/images_processed/\",transforms=trans)\n",
    "train_sampler = SequentialSampler(train_dataset)\n",
    "train_loader = DataLoader(train_dataset,batch_size=1,sampler=train_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define model and its optimizer ####\n",
    "from torch.optim import Adam\n",
    "torch.manual_seed(42)\n",
    "conv = ConvNet()\n",
    "enc = rowEncoder(64,128)\n",
    "optim = Adam(conv.parameters(),lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([13, 256])\n"
     ]
    }
   ],
   "source": [
    "### Training Loop ####\n",
    "count = 0\n",
    "\n",
    "for num,batch in enumerate(train_loader):\n",
    "    img,label,encoding = batch[\"img\"],batch[\"label\"],batch[\"encoding\"]\n",
    "    output = conv(img)\n",
    "    \n",
    "    output = output.squeeze(0)\n",
    "    output = output.permute(2,1,0)\n",
    "    h = enc.init_hidden(batch_size=13)\n",
    "    outputs,hidden = enc(output,h)\n",
    "    print(hidden.size())\n",
    "    \n",
    "    dec_hidden = dec.init_hidden()\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 13, 256])"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2index = {}\n",
    "for i in range(len(vocab)):\n",
    "    word2index[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 0\n",
    "for exp in train_df.latex_exp:\n",
    "    if len(exp.split())>max_len:\n",
    "        max_len=len(exp.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'arr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-559abb0b39d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'arr' is not defined"
     ]
    }
   ],
   "source": [
    "arr.unsqueeze(2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 4, 4])"
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr.repeat(5,1,1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "499"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
